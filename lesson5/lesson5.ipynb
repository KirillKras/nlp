{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from natasha import Segmenter, Doc, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser\n",
    "from razdel import tokenize\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "   Rating                                            Content\n0       5                                     It just works!\n1       4  –í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è...\n2       5                                        –û—Ç–ª–∏—á–Ω–æ –≤—Å–µ\n3       5  –°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ ...\n4       5                     –û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rating</th>\n      <th>Content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>It just works!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>–í —Ü–µ–ª–æ–º —É–¥–æ–±–Ω–æ–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ...–∏–∑ –º–∏–Ω—É—Å–æ–≤ —Ö–æ—Ç—è...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>–û—Ç–ª–∏—á–Ω–æ –≤—Å–µ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>–°—Ç–∞–ª –∑–∞–≤–∏—Å–∞—Ç—å –Ω–∞ 1% —Ä–∞–±–æ—Ç—ã –∞–Ω—Ç–∏–≤–∏—Ä—É—Å–∞. –î–∞–ª—å—à–µ ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>–û—á–µ–Ω—å —É–¥–æ–±–Ω–æ, —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('leto.xls', usecols=[0,1])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df.loc[df['Rating'] <= 3, 'Rating'] = 0\n",
    "df.loc[df['Rating'] > 3, 'Rating'] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16524,) (4132,) (16524,) (4132,)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(df['Content'], df['Rating'], train_size=0.8)\n",
    "print(train_X.shape, test_X.shape, train_y.shape, test_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "emoticon_list = {\n",
    "    'üëç': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòé': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòÅ': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòä': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòÄ': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòá': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòç': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòô': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòâ': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üî•': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üëå': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üòò': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üëè': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üôå': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üíã': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üí™': ' –Ω—Ä–∞–≤–∏—Ç—Å—è ',\n",
    "    'üëé': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    'üòñ': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    'üòë': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    'üòà': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    'üò°': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    'üôÖ': ' –ø–ª–æ—Ö–æ–µ ',\n",
    "    ':\\)': ' ',\n",
    "    ':\\(': ' ',\n",
    "    ';\\)': ' '\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "sw = set(get_stop_words(\"ru\"))\n",
    "morpher = MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(txt):\n",
    "    txt = str(txt).lower()\n",
    "    for emoji, word in emoticon_list.items():\n",
    "        txt = txt.replace(emoji, word)\n",
    "    txt = re.sub(r'[^\\w\\s]', ' ', txt)\n",
    "    txt = re.sub(r'[^–∞-—è–ê-–Øa-zA-Z]', ' ', txt)\n",
    "    txt = ' '.join([w for w in txt.split() if len(w) > 1])\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split()]# if word not in sw]\n",
    "    return \" \".join(txt)\n",
    "\n",
    "train_X = train_X.apply(preprocess_text)\n",
    "test_X = test_X.apply(preprocess_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_corpus = \" \".join(train_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    },
    {
     "data": {
      "text/plain": "125117"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(train_corpus)\n",
    "len(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "max_words = 200\n",
    "from nltk.probability import FreqDist\n",
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "max_len = 30\n",
    "\n",
    "train_X = np.asarray([text_to_sequence(text, max_len) for text in train_X], dtype=np.int32)\n",
    "test_X = np.asarray([text_to_sequence(text, max_len) for text in test_X], dtype=np.int32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}